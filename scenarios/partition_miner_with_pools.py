#!/usr/bin/env python3

"""
Partition Mining Scenario with Dynamic Pool + Economic Strategy

Integration of:
- Phase 1: Price Oracle (price tracking)
- Phase 2: Fee Oracle (fee market dynamics)
- Phase 3: Mining Pool Strategy (ideology + profitability)
- Phase 4: Economic Node Strategy (all nodes have ideology, dynamic economic weight)

Key changes:
- Hashrate allocation is DYNAMIC (pools switch based on profitability + ideology)
- Economic weight is DYNAMIC (economic/user nodes switch based on price + ideology)
- Network topology stays STATIC (v27 nodes isolated from v26 nodes)
- Both MINING PROBABILITY and ECONOMIC WEIGHT change over time

Feedback loop:
  Price Oracle → Pool decisions (hashrate changes)
       ↑          Economic/User decisions (economic weight changes)
       └──────────────────────┘

Network stays partitioned:
  v27 nodes ✗✗✗ v26 nodes (can't communicate)

But actors can switch which fork they support:
  Pools: choose which partition to mine on
  Economic nodes: choose which fork's economy to participate in
  User nodes: choose which fork to use
"""

from time import sleep, time
from random import random, choices
import argparse
import yaml
from pathlib import Path
from typing import Dict, List, Optional, Tuple

from commander import Commander

# Import from lib modules (bundled with scenario)
import os
from lib.price_oracle import PriceOracle
from lib.fee_oracle import FeeOracle
from lib.mining_pool_strategy import (
    ForkPreference,
    PoolProfile,
    MiningPoolStrategy,
    load_pools_from_config,
)
from lib.economic_node_strategy import (
    EconomicNodeStrategy,
    EconomicNodeProfile,
    load_economic_nodes_from_network,
)
from lib.difficulty_oracle import DifficultyOracle
from lib.reorg_oracle import ReorgOracle


class PartitionMinerWithPools(Commander):
    """
    Partition mining with dynamic pool + economic strategy.

    All actors make independent fork decisions:
    - Mining pools: choose which fork to mine (hashrate allocation)
    - Economic nodes: choose which fork's economy to support (economic weight)
    - User nodes: choose which fork to use (economic weight, small contribution)

    Decisions are based on:
    1. Profitability / Price advantage (rational economics)
    2. Ideology (fork preference)
    3. Loss tolerance / Inertia (switching costs)
    """

    def set_test_params(self):
        """Initialize test parameters"""
        self.num_nodes = 0
        self.v27_nodes = []
        self.v26_nodes = []
        self.blocks_mined = {'v27': 0, 'v26': 0}

        # Oracles
        self.price_oracle = None
        self.fee_oracle = None
        self.pool_strategy = None
        self.economic_strategy = None
        self.difficulty_oracle = None
        self.reorg_oracle = None

        # Difficulty mode tick interval
        self.tick_interval = 1.0

        # Current hashrate allocation (dynamic - from pool decisions)
        self.current_v27_hashrate = 0.0
        self.current_v26_hashrate = 0.0

        # Current economic allocation (dynamic - from economic/user node decisions)
        self.current_v27_economic = 0.0
        self.current_v26_economic = 0.0

        # Transactional economic allocation (fee-generating activity only)
        # Distinguishes exchanges/merchants (high velocity) from HODLers (low velocity)
        self.current_v27_transactional = 0.0
        self.current_v26_transactional = 0.0

        # Solo miner hashrate (user/economic nodes that mine)
        self.current_v27_solo_hashrate = 0.0
        self.current_v26_solo_hashrate = 0.0

        # Pool-to-node mappings (per partition)
        self.pool_nodes_v27 = {}  # pool_id -> list of v27 nodes
        self.pool_nodes_v26 = {}  # pool_id -> list of v26 nodes

        # Node metadata from network.yaml
        self.node_metadata = {}  # node_name -> metadata dict

        # Asymmetric fork: nodes that accept blocks from the foreign fork
        # v26 nodes have accepts_foreign_blocks=True and receive v27 blocks via submitblock
        self.foreign_accepting_nodes = []

        # Dynamic partition switching: track peer lists and node partition assignments
        self.v27_peer_addresses = []  # List of v27 node addresses for reconnection
        self.v26_peer_addresses = []  # List of v26 node addresses for reconnection
        self.node_current_partition = {}  # node_name -> current partition ('v27' or 'v26')
        self.partition_switch_history = []  # Track all partition switches for analysis

        # Time series data for charting
        self.time_series = {
            'timestamps': [],           # elapsed seconds
            'v27_price': [],
            'v26_price': [],
            'v27_hashrate': [],
            'v26_hashrate': [],
            'v27_economic': [],
            'v26_economic': [],
            'v27_blocks': [],
            'v26_blocks': [],
            'v27_difficulty': [],
            'v26_difficulty': [],
            'v27_chainwork': [],
            'v26_chainwork': [],
            # Fee market metrics
            'v27_fee_rate': [],         # sats/vbyte
            'v26_fee_rate': [],
            'v27_fee_revenue_btc': [],  # fee revenue per block
            'v26_fee_revenue_btc': [],
            'v27_congestion': [],       # tx_volume / throughput ratio
            'v26_congestion': [],
            'v27_mempool_mb': [],       # estimated mempool size
            'v26_mempool_mb': [],
            # Transactional vs custodial activity
            'v27_transactional': [],    # fee-generating economic activity %
            'v26_transactional': [],
            # Solo miner hashrate (user nodes that mine)
            'v27_solo_hashrate': [],
            'v26_solo_hashrate': [],
        }

    def add_options(self, parser: argparse.ArgumentParser):
        """Add command-line arguments"""
        parser.add_argument('--v27-economic', type=float, default=70.0,
                          help='Economic weight on v27 (0-100)')
        parser.add_argument('--v26-economic', type=float, default=None)
        parser.add_argument('--interval', type=int, default=10,
                          help='Block mining interval (seconds)')
        parser.add_argument('--duration', type=int, default=7200,
                          help='Test duration (seconds), default 2 hours')
        parser.add_argument('--start-height', type=int, default=101)

        # Pool configuration
        parser.add_argument('--pool-scenario', type=str, default='realistic_current',
                          help='Pool scenario from mining_pools_config.yaml')
        parser.add_argument('--initial-v27-hashrate', type=float, default=None,
                          help='Initial v27 hashrate (if not using pools)')

        # Network configuration
        parser.add_argument('--network-yaml', type=str, default=None,
                          help='Path to network.yaml file with node metadata')

        # Economic node configuration
        parser.add_argument('--economic-scenario', type=str, default='realistic_current',
                          help='Economic node scenario from economic_nodes_config.yaml')
        parser.add_argument('--economic-update-interval', type=int, default=300,
                          help='Economic node decision interval (seconds), default 5min')

        # Update intervals
        parser.add_argument('--hashrate-update-interval', type=int, default=600,
                          help='Pool decision interval (seconds), default 10min')
        parser.add_argument('--price-update-interval', type=int, default=60,
                          help='Price update interval (seconds)')

        # Difficulty oracle configuration
        parser.add_argument('--enable-difficulty', action='store_true', default=False,
                          help='Enable difficulty simulation (probability-per-tick mining)')
        parser.add_argument('--retarget-interval', type=int, default=144,
                          help='Blocks between difficulty adjustments (default 144)')
        parser.add_argument('--tick-interval', type=float, default=1.0,
                          help='Tick interval in seconds for difficulty mode (default 1.0)')
        parser.add_argument('--enable-eda', action='store_true', default=False,
                          help='Enable Emergency Difficulty Adjustment (BCH-style)')
        parser.add_argument('--min-difficulty', type=float, default=0.0625,
                          help='Minimum difficulty floor (default 0.0625 = 1/16)')

        # Reorg metrics
        parser.add_argument('--enable-reorg-metrics', action='store_true', default=False,
                          help='Enable reorg tracking oracle for fork impact metrics')
        parser.add_argument('--enable-dynamic-switching', action='store_true', default=False,
                          help='Enable dynamic partition switching for economic/user nodes')

        # Fork reunion
        parser.add_argument('--enable-reunion', action='store_true', default=False,
                          help='At end of duration, reconnect partitions and let the heavier chain reorg the loser')
        parser.add_argument('--reunion-timeout', type=int, default=120,
                          help='Seconds to wait for reorg convergence after reconnection (default 120)')

        # Results management
        parser.add_argument('--results-id', type=str, default=None,
                          help='Unique identifier for this run (default: auto-generated timestamp)')
        parser.add_argument('--snapshot-interval', type=int, default=60,
                          help='Interval in seconds for time series snapshots (default: 60)')

        # Debug options
        parser.add_argument('--debug-prices', action='store_true', default=False,
                          help='Enable verbose price calculation debugging')

    def load_network_metadata(self):
        """Load node metadata from network.yaml file or bundled config"""
        network_config = None

        # Try loading from specified path first
        if self.options.network_yaml:
            network_yaml_path = Path(self.options.network_yaml)
            if network_yaml_path.exists():
                self.log.info(f"Loading network metadata from {network_yaml_path}")
                try:
                    with open(network_yaml_path, 'r') as f:
                        network_config = yaml.safe_load(f)
                except Exception as e:
                    self.log.warning(f"Error loading network YAML from path: {e}")

        # Fallback: try loading from bundled config
        if network_config is None:
            try:
                import pkgutil
                bundled_data = pkgutil.get_data('config', 'network_metadata.yaml')
                if bundled_data:
                    network_config = yaml.safe_load(bundled_data.decode('utf-8'))
                    self.log.info("Loading network metadata from bundled config")
            except Exception as e:
                self.log.debug(f"No bundled network metadata: {e}")

        # If we loaded config, check for accepts_foreign_blocks and infer from version if missing
        if network_config:
            nodes = network_config.get('nodes', [])
            foreign_blocks_set = any(
                n.get('metadata', {}).get('accepts_foreign_blocks') is not None
                for n in nodes
            )
            if not foreign_blocks_set:
                self.log.info("Inferring accepts_foreign_blocks from node versions (v26 nodes accept v27 blocks)")
                for node_config in nodes:
                    image_tag = node_config.get('image', {}).get('tag', '')
                    metadata = node_config.get('metadata', {})
                    # v26 nodes accept v27 blocks (non-contentious soft fork model)
                    if '26' in str(image_tag):
                        metadata['accepts_foreign_blocks'] = True
                    else:
                        metadata['accepts_foreign_blocks'] = False
                    node_config['metadata'] = metadata

        if network_config is None:
            self.log.warning("No network metadata available, pool/economic mapping will be limited")
            return

        try:
            # Build node name -> metadata mapping
            for node_config in network_config.get('nodes', []):
                node_name = node_config.get('name')
                metadata = node_config.get('metadata', {})

                if node_name:
                    self.node_metadata[node_name] = metadata

            self.log.info(f"✓ Loaded metadata for {len(self.node_metadata)} nodes")

        except Exception as e:
            self.log.error(f"Error loading network YAML: {e}")

    def partition_nodes_by_version(self):
        """Separate nodes into v27 and v26 partitions"""
        self.log.info("Partitioning nodes by version...")

        for node in self.nodes:
            try:
                network_info = node.getnetworkinfo()
                version_string = network_info.get('subversion', '')

                is_v27 = '27.' in version_string or ':27.' in version_string
                is_v26 = '26.' in version_string or ':26.' in version_string

                if is_v27:
                    self.v27_nodes.append(node)
                elif is_v26:
                    self.v26_nodes.append(node)

            except Exception as e:
                self.log.error(f"  Error querying node {node.index}: {e}")

        self.log.info(f"\nPartition summary:")
        self.log.info(f"  v27 nodes: {len(self.v27_nodes)}")
        self.log.info(f"  v26 nodes: {len(self.v26_nodes)}")

    def build_foreign_accepting_nodes(self):
        """
        Identify nodes that accept blocks from the foreign fork (accepts_foreign_blocks=True).

        In the asymmetric fork model:
          - v27 (strict): rejects v26 blocks — accepts_foreign_blocks=False
          - v26 (permissive): accepts v27 blocks — accepts_foreign_blocks=True

        When a v27 block is mined, it is submitted to the first foreign-accepting node
        via submitblock. P2P propagation within the v26 island handles the rest.

        Note: v26 nodes automatically accept v27 blocks in non-contentious soft fork model,
        so we treat all v26 nodes as foreign-accepting by default.
        """
        self.foreign_accepting_nodes = []

        for node in self.v26_nodes:
            node_name = f"node-{node.index:04d}"
            metadata = self.node_metadata.get(node_name, {})

            # Check explicit flag first
            accepts_foreign = metadata.get('accepts_foreign_blocks', None)

            # If not explicitly set, v26 nodes accept v27 blocks by default
            # (non-contentious soft fork model)
            if accepts_foreign is None:
                # v26 nodes accept v27 blocks (stricter rules are valid under looser rules)
                accepts_foreign = True
                self.log.debug(f"  {node_name}: accepts_foreign_blocks not set, defaulting to True (v26 node)")

            if accepts_foreign:
                self.foreign_accepting_nodes.append(node)

        if self.foreign_accepting_nodes:
            self.log.info(
                f"\nAsymmetric fork mode: {len(self.foreign_accepting_nodes)} foreign-accepting node(s). "
                f"v27 blocks will be submitted to node-{self.foreign_accepting_nodes[0].index:04d} "
                f"for v26-island propagation."
            )
        else:
            self.log.info(
                "\nAsymmetric fork mode: no foreign-accepting nodes found "
                "(accepts_foreign_blocks not set in metadata). "
                "Running as standard symmetric partition."
            )

    def propagate_to_foreign_accepting(self, miner, fork_id: str):
        """
        After mining a v27 block, push it to the v26 island via submitblock.

        Only acts when fork_id=='v27' and foreign-accepting nodes exist.
        Submits to one bridge node only — P2P handles intra-island propagation.
        """
        if fork_id != 'v27' or not self.foreign_accepting_nodes:
            return

        try:
            block_hash = miner.getbestblockhash()
            raw_block = miner.getblock(block_hash, 0)
            bridge_node = self.foreign_accepting_nodes[0]
            result = bridge_node.submitblock(raw_block)
            if result is not None:
                # submitblock returns None on success; any other value is a rejection reason
                self.log.warning(
                    f"  [asymmetric] submitblock to node-{bridge_node.index:04d} returned: {result}"
                )
        except Exception as e:
            self.log.error(f"  [asymmetric] Failed to propagate v27 block to v26 island: {e}")

    def build_partition_peer_lists(self):
        """
        Build lists of peer addresses for each partition.
        Used for dynamic partition switching - nodes can reconnect to a different partition.
        """
        self.log.info("\nBuilding partition peer lists for dynamic switching...")

        # Get addresses from nodes in each partition
        for node in self.v27_nodes:
            try:
                # Get the node's address that other nodes can connect to
                node_name = f"node-{node.index:04d}"
                # In warnet, nodes are addressable by their service name
                self.v27_peer_addresses.append(node_name)
                self.node_current_partition[node_name] = 'v27'
            except Exception as e:
                self.log.warning(f"  Could not get address for v27 node {node.index}: {e}")

        for node in self.v26_nodes:
            try:
                node_name = f"node-{node.index:04d}"
                self.v26_peer_addresses.append(node_name)
                self.node_current_partition[node_name] = 'v26'
            except Exception as e:
                self.log.warning(f"  Could not get address for v26 node {node.index}: {e}")

        self.log.info(f"  v27 peers: {len(self.v27_peer_addresses)} nodes")
        self.log.info(f"  v26 peers: {len(self.v26_peer_addresses)} nodes")

    def get_node_peers(self, node) -> list:
        """Get list of currently connected peer addresses for a node."""
        try:
            peer_info = node.getpeerinfo()
            return [p.get('addr', '').split(':')[0] for p in peer_info]
        except Exception as e:
            self.log.warning(f"  Could not get peers for node {node.index}: {e}")
            return []

    def switch_node_partition(self, node, old_partition: str, new_partition: str, reason: str = "") -> bool:
        """
        Switch a node from one partition to another by changing P2P connections.

        Steps:
        1. Disconnect from old partition peers
        2. Connect to new partition peers
        3. Wait for sync to new chain
        4. Update tracking

        Args:
            node: The node to switch
            old_partition: Current partition ('v27' or 'v26')
            new_partition: Target partition ('v27' or 'v26')
            reason: Why the switch is happening (for logging)

        Returns:
            True if switch was successful
        """
        if old_partition == new_partition:
            return True  # No switch needed

        node_name = f"node-{node.index:04d}"
        self.log.info(f"\n  PARTITION SWITCH: {node_name} {old_partition} -> {new_partition}")
        if reason:
            self.log.info(f"    Reason: {reason}")

        old_peers = self.v27_peer_addresses if old_partition == 'v27' else self.v26_peer_addresses
        new_peers = self.v26_peer_addresses if new_partition == 'v26' else self.v27_peer_addresses

        try:
            # Step 1: Get current height before switch
            old_height = node.getblockcount()
            old_hash = node.getbestblockhash()

            # Step 2: Disconnect from old partition peers
            current_peers = self.get_node_peers(node)
            disconnected = 0
            for peer_addr in current_peers:
                # Check if this peer is in the old partition
                for old_peer in old_peers:
                    if old_peer in peer_addr:
                        try:
                            node.disconnectnode(peer_addr)
                            disconnected += 1
                        except Exception:
                            pass  # May already be disconnected
                        break

            self.log.info(f"    Disconnected from {disconnected} {old_partition} peers")

            # Step 3: Connect to new partition peers
            connected = 0
            # Connect to a subset of new peers (not all, to avoid overwhelming)
            peers_to_add = new_peers[:min(8, len(new_peers))]
            for peer_name in peers_to_add:
                if peer_name != node_name:  # Don't connect to self
                    try:
                        node.addnode(peer_name, "onetry")
                        connected += 1
                    except Exception as e:
                        self.log.warning(f"    Could not connect to {peer_name}: {e}")

            self.log.info(f"    Connected to {connected} {new_partition} peers")

            # Step 4: Wait briefly for connection establishment
            import time
            time.sleep(2)

            # Step 5: Check if we synced to new chain
            new_height = node.getblockcount()
            new_hash = node.getbestblockhash()

            if new_hash != old_hash:
                self.log.info(f"    Chain changed: height {old_height} -> {new_height}")
                self.log.info(f"    Old tip: {old_hash[:16]}...")
                self.log.info(f"    New tip: {new_hash[:16]}...")

            # Step 6: Update tracking
            self.node_current_partition[node_name] = new_partition

            # Move node between partition lists
            if old_partition == 'v27' and node in self.v27_nodes:
                self.v27_nodes.remove(node)
                self.v26_nodes.append(node)
            elif old_partition == 'v26' and node in self.v26_nodes:
                self.v26_nodes.remove(node)
                self.v27_nodes.append(node)

            # Record the switch
            self.partition_switch_history.append({
                'node': node_name,
                'from': old_partition,
                'to': new_partition,
                'reason': reason,
                'old_height': old_height,
                'new_height': new_height,
                'timestamp': time.time()
            })

            self.log.info(f"    Switch complete: {node_name} is now on {new_partition}")
            return True

        except Exception as e:
            self.log.error(f"    Switch failed for {node_name}: {e}")
            return False

    def reunite_forks(self) -> dict:
        """
        Reconnect the two fork partitions and wait for the losing fork to reorg
        to the heavier chain.

        The winning fork is determined by cumulative chainwork (difficulty oracle)
        or by block count if difficulty tracking is disabled.

        Steps:
        1. Determine the winning fork by chainwork / block count
        2. Connect each losing-fork node to several winning-fork peers via addnode
        3. Poll until all losing-fork nodes converge to the winning tip (or timeout)
        4. Report reorg depth, orphaned blocks, and convergence metrics

        Returns:
            dict with reunion metrics (included in the results export)
        """
        if not self.options.enable_reunion:
            return {'enabled': False}

        self.log.info(f"\n{'='*70}")
        self.log.info("FORK REUNION")
        self.log.info(f"{'='*70}")

        # Determine winning and losing forks
        if self.difficulty_oracle:
            winner_id, winner_cw, loser_cw = self.difficulty_oracle.get_winning_fork()
        else:
            winner_id = 'v27' if self.blocks_mined['v27'] >= self.blocks_mined['v26'] else 'v26'
            winner_cw = float(self.blocks_mined[winner_id])
            loser_cw = float(self.blocks_mined['v26' if winner_id == 'v27' else 'v27'])

        loser_id = 'v26' if winner_id == 'v27' else 'v27'
        winning_nodes = self.v27_nodes if winner_id == 'v27' else self.v26_nodes
        losing_nodes = self.v26_nodes if winner_id == 'v27' else self.v27_nodes
        winning_peers = self.v27_peer_addresses if winner_id == 'v27' else self.v26_peer_addresses

        if not winning_nodes or not losing_nodes:
            self.log.warning("  Cannot reunite: one partition has no nodes")
            return {'enabled': True, 'skipped': True, 'reason': 'empty_partition'}

        # Snapshot pre-reorg state
        try:
            winning_tip_hash = winning_nodes[0].getbestblockhash()
            winning_tip_height = winning_nodes[0].getblockcount()
        except Exception as e:
            self.log.error(f"  Cannot read winning fork tip: {e}")
            return {'enabled': True, 'skipped': True, 'reason': 'rpc_error'}

        pre_reorg_heights = {}
        for node in losing_nodes:
            node_name = f"node-{node.index:04d}"
            try:
                pre_reorg_heights[node_name] = node.getblockcount()
            except Exception as e:
                self.log.warning(f"  Could not get height for {node_name}: {e}")
                pre_reorg_heights[node_name] = 0

        losing_tip_height = max(pre_reorg_heights.values(), default=0)
        self.log.info(f"  Winning fork: {winner_id}  height={winning_tip_height}  chainwork={winner_cw:.4f}")
        self.log.info(f"  Losing fork:  {loser_id}  height={losing_tip_height}  chainwork={loser_cw:.4f}")
        self.log.info(f"  Connecting {len(losing_nodes)} {loser_id} nodes to {winner_id} peers...")

        # Connect each losing-fork node to up to 4 winning-fork peers
        peers_to_add = winning_peers[:min(4, len(winning_peers))]
        connected_count = 0
        for node in losing_nodes:
            node_name = f"node-{node.index:04d}"
            for peer_name in peers_to_add:
                if peer_name != node_name:
                    try:
                        node.addnode(peer_name, "onetry")
                        connected_count += 1
                    except Exception as e:
                        self.log.debug(f"  Could not connect {node_name} -> {peer_name}: {e}")

        self.log.info(f"  Added {connected_count} cross-partition connections")
        self.log.info(f"  Waiting up to {self.options.reunion_timeout}s for reorg convergence...")

        # Poll for convergence
        poll_interval = 2.0
        reunion_start = time()
        converged_nodes = set()
        wait_elapsed = 0.0

        while True:
            wait_elapsed = time() - reunion_start
            if wait_elapsed >= self.options.reunion_timeout:
                break

            # Refresh winning tip in case mining is still active
            try:
                winning_tip_hash = winning_nodes[0].getbestblockhash()
                winning_tip_height = winning_nodes[0].getblockcount()
            except Exception:
                pass

            for node in losing_nodes:
                node_name = f"node-{node.index:04d}"
                if node_name in converged_nodes:
                    continue
                try:
                    best_hash = node.getbestblockhash()
                    if best_hash == winning_tip_hash:
                        post_h = node.getblockcount()
                        self.log.info(f"  {node_name} converged at height {post_h} ({wait_elapsed:.1f}s)")
                        converged_nodes.add(node_name)
                except Exception as e:
                    self.log.debug(f"  Polling {node_name}: {e}")

            if len(converged_nodes) >= len(losing_nodes):
                break

            sleep(poll_interval)

        # Final heights after waiting
        post_reorg_heights = {}
        for node in losing_nodes:
            node_name = f"node-{node.index:04d}"
            try:
                post_reorg_heights[node_name] = node.getblockcount()
            except Exception:
                post_reorg_heights[node_name] = pre_reorg_heights.get(node_name, 0)

        # Metrics
        fork_point_height = self.options.start_height
        losing_fork_depth = losing_tip_height - fork_point_height
        orphaned_blocks = self.blocks_mined[loser_id]
        nodes_converged = len(converged_nodes)
        nodes_total = len(losing_nodes)
        timed_out = nodes_converged < nodes_total

        self.log.info(f"\n  REUNION RESULTS:")
        self.log.info(f"  Winner:          {winner_id} (height {winning_tip_height})")
        self.log.info(f"  Reorg depth:     {losing_fork_depth} blocks ({loser_id} fork above fork point)")
        self.log.info(f"  Orphaned blocks: {orphaned_blocks}")
        self.log.info(f"  Nodes converged: {nodes_converged}/{nodes_total} in {wait_elapsed:.1f}s")
        if timed_out:
            self.log.warning(f"  WARNING: {nodes_total - nodes_converged} nodes did not converge within timeout")

        return {
            'enabled': True,
            'winner': winner_id,
            'loser': loser_id,
            'winner_chainwork': winner_cw,
            'loser_chainwork': loser_cw,
            'winning_tip_height': winning_tip_height,
            'winning_tip_hash': winning_tip_hash,
            'fork_point_height': fork_point_height,
            'losing_fork_depth': losing_fork_depth,
            'orphaned_blocks': orphaned_blocks,
            'nodes_converged': nodes_converged,
            'nodes_total': nodes_total,
            'timed_out': timed_out,
            'wait_elapsed_seconds': round(wait_elapsed, 1),
            'pre_reorg_heights': pre_reorg_heights,
            'post_reorg_heights': post_reorg_heights,
        }

    def evaluate_economic_node_switches(self, elapsed: float):
        """
        Evaluate whether economic/user nodes should switch partitions based on
        economic conditions (price, fees, ideology).

        Called periodically during simulation to allow dynamic partition changes.
        """
        if not self.economic_strategy:
            return

        switches = []

        # Get current prices and conditions
        v27_price = self.price_oracle.get_price('v27') if self.price_oracle else 60000
        v26_price = self.price_oracle.get_price('v26') if self.price_oracle else 60000
        price_ratio = v27_price / v26_price if v26_price > 0 else 1.0

        # Evaluate each economic node
        for node in list(self.v27_nodes) + list(self.v26_nodes):
            node_name = f"node-{node.index:04d}"
            metadata = self.node_metadata.get(node_name, {})

            # Skip pool nodes - they have their own switching logic
            if metadata.get('node_type') == 'mining_pool':
                continue

            # Only evaluate economic and user nodes
            node_type = metadata.get('node_type', '')
            if node_type not in ['economic', 'user']:
                continue

            current_partition = self.node_current_partition.get(node_name, 'v27')
            fork_preference = metadata.get('fork_preference', 'neutral')
            ideology_strength = metadata.get('ideology_strength', 0.0)
            switching_threshold = metadata.get('switching_threshold', 0.10)
            inertia = metadata.get('inertia', 0.05)

            # Determine if node should switch
            should_switch = False
            reason = ""

            if current_partition == 'v27':
                # On v27, consider switching to v26 if:
                # 1. v26 has significantly better price AND low ideology
                # 2. Or fork_preference is v26 with high ideology
                if fork_preference == 'v26' and ideology_strength > 0.7:
                    should_switch = True
                    reason = f"ideological preference for v26 (strength={ideology_strength:.2f})"
                elif price_ratio < (1 - switching_threshold) and ideology_strength < 0.3:
                    should_switch = True
                    reason = f"economic: v27 price {price_ratio:.2%} below v26"

            else:  # current_partition == 'v26'
                # On v26, consider switching to v27 if:
                # 1. v27 has significantly better price AND low ideology
                # 2. Or fork_preference is v27 with high ideology
                if fork_preference == 'v27' and ideology_strength > 0.7:
                    should_switch = True
                    reason = f"ideological preference for v27 (strength={ideology_strength:.2f})"
                elif price_ratio > (1 + switching_threshold) and ideology_strength < 0.3:
                    should_switch = True
                    reason = f"economic: v27 price {price_ratio:.2%} above v26"

            # Apply inertia - random chance to delay switch
            if should_switch:
                import random
                if random.random() < inertia:
                    should_switch = False
                    self.log.debug(f"  {node_name} switch delayed by inertia")

            if should_switch:
                new_partition = 'v26' if current_partition == 'v27' else 'v27'
                switches.append((node, current_partition, new_partition, reason))

        # Execute switches
        for node, old_part, new_part, reason in switches:
            self.switch_node_partition(node, old_part, new_part, reason)

    def build_pool_node_mapping(self):
        """
        Build mapping from pool IDs to nodes in each partition.

        Architecture:
        - Each pool has ONE node per partition (if present)
        - Same entity_id appears in both partitions (paired nodes)
        - Pool decides which fork to mine → uses corresponding node
        """
        self.log.info("\nBuilding pool-to-node mappings...")

        # Track unmapped nodes
        v27_unmapped = 0
        v26_unmapped = 0

        # Read node metadata to extract pool assignments
        for node in self.v27_nodes:
            pool_id = self.get_node_pool_id(node)
            if pool_id:
                if pool_id not in self.pool_nodes_v27:
                    self.pool_nodes_v27[pool_id] = []
                self.pool_nodes_v27[pool_id].append(node)
            else:
                v27_unmapped += 1

        for node in self.v26_nodes:
            pool_id = self.get_node_pool_id(node)
            if pool_id:
                if pool_id not in self.pool_nodes_v26:
                    self.pool_nodes_v26[pool_id] = []
                self.pool_nodes_v26[pool_id].append(node)
            else:
                v26_unmapped += 1

        # Log pool distribution
        self.log.info("\nPool node distribution (1 node per partition per pool):")

        # Get all unique pool IDs from both partitions
        all_pools = set(self.pool_nodes_v27.keys()) | set(self.pool_nodes_v26.keys())

        for pool_id in sorted(all_pools):
            v27_nodes = self.pool_nodes_v27.get(pool_id, [])
            v26_nodes = self.pool_nodes_v26.get(pool_id, [])

            v27_str = f"node-{v27_nodes[0].index:04d}" if v27_nodes else "none"
            v26_str = f"node-{v26_nodes[0].index:04d}" if v26_nodes else "none"

            # Warn if pool has multiple nodes in one partition (unexpected)
            if len(v27_nodes) > 1:
                self.log.warning(f"  {pool_id:15s}: MULTIPLE v27 nodes ({len(v27_nodes)}) - expected 1")
            if len(v26_nodes) > 1:
                self.log.warning(f"  {pool_id:15s}: MULTIPLE v26 nodes ({len(v26_nodes)}) - expected 1")

            self.log.info(f"  {pool_id:15s}: v27={v27_str:12s} v26={v26_str:12s}")

        if v27_unmapped > 0:
            self.log.info(f"\n  {v27_unmapped} v27 nodes without pool assignment (economic/user nodes)")

        if v26_unmapped > 0:
            self.log.info(f"  {v26_unmapped} v26 nodes without pool assignment (economic/user nodes)")

    def get_node_pool_id(self, node) -> Optional[str]:
        """Extract pool ID from node metadata (read from network.yaml)"""
        try:
            # Get node name from tank_index
            # Warnet nodes are typically named "node-XXXX"
            node_name = f"node-{node.index:04d}"

            # Look up metadata from network.yaml
            if node_name in self.node_metadata:
                metadata = self.node_metadata[node_name]
                entity_id = metadata.get('entity_id', None)

                if entity_id and entity_id.startswith('pool-'):
                    # Convert pool-antpool -> antpool
                    pool_id = entity_id.replace('pool-', '')
                    return pool_id

            return None

        except Exception as e:
            self.log.debug(f"Could not extract pool ID from node {node.index}: {e}")
            return None

    def select_mining_node(self) -> Tuple[Optional[object], Optional[str]]:
        """
        Select a mining node based on pool decisions and hashrate weights.

        Architecture (Simple Binary Allocation):
        1. Each pool chooses ONE fork to mine ('v27' or 'v26')
        2. Pool's full hashrate goes to chosen fork
        3. Weighted random selection based on hashrate
        4. Selected pool's node on chosen partition mines the block
        """
        if not self.pool_strategy:
            # Fallback: random selection from aggregate hashrate
            rand_val = random() * 100.0
            if rand_val < self.current_v27_hashrate:
                partition = 'v27'
                nodes = self.v27_nodes
            else:
                partition = 'v26'
                nodes = self.v26_nodes

            if not nodes:
                return None, None
            return choices(nodes, k=1)[0], partition

        # Pool-based selection with binary allocation
        # Build weighted list of (pool_id, partition) based on pool decisions
        pool_choices = []
        pool_weights = []

        for pool_id, allocation in self.pool_strategy.current_allocation.items():
            pool = self.pool_strategy.pools[pool_id]
            hashrate_weight = pool.hashrate_pct

            # Binary allocation: pool chooses one fork
            chosen_fork = allocation  # 'v27' or 'v26'

            # Check if pool has a node in the chosen partition
            if chosen_fork == 'v27':
                if pool_id in self.pool_nodes_v27 and self.pool_nodes_v27[pool_id]:
                    pool_choices.append((pool_id, 'v27'))
                    pool_weights.append(hashrate_weight)
                else:
                    self.log.debug(f"Pool {pool_id} chose v27 but has no v27 node")
            elif chosen_fork == 'v26':
                if pool_id in self.pool_nodes_v26 and self.pool_nodes_v26[pool_id]:
                    pool_choices.append((pool_id, 'v26'))
                    pool_weights.append(hashrate_weight)
                else:
                    self.log.debug(f"Pool {pool_id} chose v26 but has no v26 node")

        if not pool_choices:
            # No pool nodes mapped, fallback to random selection based on hashrate
            rand_val = random() * 100.0
            if rand_val < self.current_v27_hashrate:
                partition = 'v27'
                nodes = self.v27_nodes
            else:
                partition = 'v26'
                nodes = self.v26_nodes

            if not nodes:
                return None, None
            return choices(nodes, k=1)[0], partition

        # Weighted random selection by hashrate
        selected_pool_id, partition = choices(pool_choices, weights=pool_weights, k=1)[0]

        # Get the pool's node on the chosen partition (should be exactly 1 node)
        if partition == 'v27':
            nodes = self.pool_nodes_v27.get(selected_pool_id, [])
        else:
            nodes = self.pool_nodes_v26.get(selected_pool_id, [])

        if not nodes:
            self.log.warning(f"No nodes for pool {selected_pool_id} in {partition}")
            return None, None

        # Return the pool's node (first/only node in list)
        selected_node = nodes[0]
        return selected_node, partition

    def _select_miner_for_fork(self, fork_id: str) -> Optional[object]:
        """
        Select a mining node for a specific fork (used in difficulty mode).

        In difficulty mode, we already know WHICH fork gets a block. This method
        selects which pool/node mines it, weighted by hashrate of pools AND solo
        miners allocated to that fork.

        Args:
            fork_id: 'v27' or 'v26'

        Returns:
            A node object to mine on, or None if no node available
        """
        nodes = self.v27_nodes if fork_id == 'v27' else self.v26_nodes
        if not nodes:
            return None

        if not self.pool_strategy:
            # No pool strategy: random node from partition
            return choices(nodes, k=1)[0]

        # Build weighted list of pools allocated to this fork
        miner_choices = []
        miner_weights = []

        # Add pools
        for pool_id, allocation in self.pool_strategy.current_allocation.items():
            if allocation != fork_id:
                continue

            pool = self.pool_strategy.pools[pool_id]
            pool_node_map = self.pool_nodes_v27 if fork_id == 'v27' else self.pool_nodes_v26

            if pool_id in pool_node_map and pool_node_map[pool_id]:
                miner_choices.append(pool_node_map[pool_id][0])
                miner_weights.append(pool.hashrate_pct)

        # Add solo miners (user/economic nodes with hashrate)
        if self.economic_strategy:
            solo_miners = self.economic_strategy.get_solo_miners()
            for node_id, miner_fork, hashrate in solo_miners:
                if miner_fork != fork_id or hashrate <= 0:
                    continue

                # Find the actual node object by node_id
                node_obj = self._get_node_by_id(node_id, fork_id)
                if node_obj:
                    miner_choices.append(node_obj)
                    miner_weights.append(hashrate)

        if not miner_choices:
            # Fallback: random node from partition
            return choices(nodes, k=1)[0]

        return choices(miner_choices, weights=miner_weights, k=1)[0]

    def _get_node_by_id(self, node_id: str, fork_id: str) -> Optional[object]:
        """
        Get a node object by its node_id string.

        Args:
            node_id: Node identifier (e.g., "node-0008")
            fork_id: Which fork partition to look in ('v27' or 'v26')

        Returns:
            Node object or None if not found
        """
        nodes = self.v27_nodes if fork_id == 'v27' else self.v26_nodes

        for node in nodes:
            # Node names are typically "node-XXXX" format
            node_name = f"node-{node.index:04d}"
            if node_name == node_id:
                return node

        return None

    def capture_time_series_snapshot(self, elapsed: int):
        """
        Capture a snapshot of current state for time series charting.

        Args:
            elapsed: Seconds since simulation start
        """
        self.time_series['timestamps'].append(elapsed)
        self.time_series['v27_price'].append(self.price_oracle.get_price('v27'))
        self.time_series['v26_price'].append(self.price_oracle.get_price('v26'))
        self.time_series['v27_hashrate'].append(self.current_v27_hashrate)
        self.time_series['v26_hashrate'].append(self.current_v26_hashrate)
        self.time_series['v27_economic'].append(self.current_v27_economic)
        self.time_series['v26_economic'].append(self.current_v26_economic)
        self.time_series['v27_blocks'].append(self.blocks_mined['v27'])
        self.time_series['v26_blocks'].append(self.blocks_mined['v26'])

        if self.difficulty_oracle:
            v27_state = self.difficulty_oracle.forks.get('v27')
            v26_state = self.difficulty_oracle.forks.get('v26')
            self.time_series['v27_difficulty'].append(
                v27_state.current_difficulty if v27_state else 1.0)
            self.time_series['v26_difficulty'].append(
                v26_state.current_difficulty if v26_state else 1.0)
            self.time_series['v27_chainwork'].append(
                v27_state.cumulative_chainwork if v27_state else 0.0)
            self.time_series['v26_chainwork'].append(
                v26_state.cumulative_chainwork if v26_state else 0.0)
        else:
            self.time_series['v27_difficulty'].append(1.0)
            self.time_series['v26_difficulty'].append(1.0)
            self.time_series['v27_chainwork'].append(float(self.blocks_mined['v27']))
            self.time_series['v26_chainwork'].append(float(self.blocks_mined['v26']))

        # Fee market metrics
        if self.fee_oracle:
            # Fee rates
            self.time_series['v27_fee_rate'].append(self.fee_oracle.get_fee('v27'))
            self.time_series['v26_fee_rate'].append(self.fee_oracle.get_fee('v26'))

            # Fee revenue per block (miner incentive from fees)
            self.time_series['v27_fee_revenue_btc'].append(
                self.fee_oracle.get_fee_revenue_per_block('v27'))
            self.time_series['v26_fee_revenue_btc'].append(
                self.fee_oracle.get_fee_revenue_per_block('v26'))

            # Mempool/congestion estimates
            # Get blocks per hour for congestion calculation
            if self.difficulty_oracle:
                v27_bph = self.difficulty_oracle.get_blocks_per_hour('v27', self.current_v27_hashrate)
                v26_bph = self.difficulty_oracle.get_blocks_per_hour('v26', self.current_v26_hashrate)
            else:
                # Estimate from hashrate (normal rate = 6 blocks/hour)
                v27_bph = 6.0 * (self.current_v27_hashrate / 100.0) if self.current_v27_hashrate > 0 else 0.1
                v26_bph = 6.0 * (self.current_v26_hashrate / 100.0) if self.current_v26_hashrate > 0 else 0.1

            v27_mempool = self.fee_oracle.estimate_mempool_size(
                'v27', v27_bph, self.current_v27_economic)
            v26_mempool = self.fee_oracle.estimate_mempool_size(
                'v26', v26_bph, self.current_v26_economic)

            self.time_series['v27_congestion'].append(v27_mempool['congestion_ratio'])
            self.time_series['v26_congestion'].append(v26_mempool['congestion_ratio'])
            self.time_series['v27_mempool_mb'].append(v27_mempool['estimated_mempool_mb'])
            self.time_series['v26_mempool_mb'].append(v26_mempool['estimated_mempool_mb'])
        else:
            # Default values if no fee oracle
            self.time_series['v27_fee_rate'].append(1.0)
            self.time_series['v26_fee_rate'].append(1.0)
            self.time_series['v27_fee_revenue_btc'].append(0.01)
            self.time_series['v26_fee_revenue_btc'].append(0.01)
            self.time_series['v27_congestion'].append(1.0)
            self.time_series['v26_congestion'].append(1.0)
            self.time_series['v27_mempool_mb'].append(0.0)
            self.time_series['v26_mempool_mb'].append(0.0)

        # Transactional weight (fee-generating economic activity)
        self.time_series['v27_transactional'].append(self.current_v27_transactional)
        self.time_series['v26_transactional'].append(self.current_v26_transactional)

        # Solo miner hashrate
        self.time_series['v27_solo_hashrate'].append(self.current_v27_solo_hashrate)
        self.time_series['v26_solo_hashrate'].append(self.current_v26_solo_hashrate)

    def run_test(self):
        """Main mining loop with dynamic pool strategy"""

        # Calculate complementary economic weight
        if self.options.v26_economic is None:
            self.options.v26_economic = 100.0 - self.options.v27_economic

        self.log.info(f"\n{'='*70}")
        self.log.info(f"Partition Mining with Dynamic Pool Strategy")
        self.log.info(f"{'='*70}")
        self.log.info(f"Initial economic weights: v27={self.options.v27_economic}%, v26={self.options.v26_economic}%")
        self.log.info(f"Duration: {self.options.duration}s ({self.options.duration/60:.0f} minutes)")
        self.log.info(f"Pool scenario: {self.options.pool_scenario}")
        self.log.info(f"Economic scenario: {self.options.economic_scenario}")
        self.log.info(f"{'='*70}\n")

        # Initialize oracles
        self.price_oracle = PriceOracle(
            base_price=60000,
            min_fork_depth=6,
            debug=self.options.debug_prices
        )
        self.log.info(f"✓ Price oracle initialized (debug={self.options.debug_prices})")

        self.fee_oracle = FeeOracle()
        self.log.info("✓ Fee oracle initialized")

        # Initialize pool strategy
        try:
            # Load config from bundled package (works inside .pyz archive)
            import pkgutil
            import yaml
            config_data = pkgutil.get_data('config', 'mining_pools_config.yaml')
            config = yaml.safe_load(config_data.decode('utf-8'))

            scenario_name = self.options.pool_scenario
            if scenario_name not in config:
                raise ValueError(f"Scenario '{scenario_name}' not found in config")

            scenario = config[scenario_name]
            pools = []
            for pool_data in scenario.get('pools', []):
                pools.append(PoolProfile(
                    pool_id=pool_data['pool_id'],
                    hashrate_pct=pool_data['hashrate_pct'],
                    fork_preference=ForkPreference(pool_data.get('fork_preference', 'neutral')),
                    ideology_strength=pool_data.get('ideology_strength', pool_data.get('ideology_score', 0.5)),
                    profitability_threshold=pool_data.get('profitability_threshold', pool_data.get('switch_threshold_pct', 5.0) / 100.0),
                    max_loss_usd=pool_data.get('max_loss_usd'),
                    max_loss_pct=pool_data.get('max_loss_pct', 0.10),
                ))
            self.pool_strategy = MiningPoolStrategy(pools)
            self.log.info(f"✓ Pool strategy initialized ({len(pools)} pools)")

            # Set initial hashrate from pool scenario
            # Assume pools start mining their preferred fork if they have one
            initial_v27 = 0.0
            initial_v26 = 0.0
            for pool in pools:
                if pool.fork_preference == ForkPreference.V27:
                    initial_v27 += pool.hashrate_pct
                elif pool.fork_preference == ForkPreference.V26:
                    initial_v26 += pool.hashrate_pct
                else:
                    # Neutral pools split evenly initially
                    initial_v27 += pool.hashrate_pct / 2
                    initial_v26 += pool.hashrate_pct / 2

            self.current_v27_hashrate = initial_v27
            self.current_v26_hashrate = initial_v26

            # Set initial allocation in pool strategy based on preferences
            # Neutral pools alternate to approximate 50/50 split
            neutral_toggle = True
            for pool in pools:
                if pool.fork_preference == ForkPreference.V27:
                    self.pool_strategy.current_allocation[pool.pool_id] = 'v27'
                elif pool.fork_preference == ForkPreference.V26:
                    self.pool_strategy.current_allocation[pool.pool_id] = 'v26'
                else:
                    # Neutral pools alternate between forks
                    self.pool_strategy.current_allocation[pool.pool_id] = 'v27' if neutral_toggle else 'v26'
                    neutral_toggle = not neutral_toggle

        except Exception as e:
            self.log.warning(f"Could not load pool config: {e}")
            self.log.info("Using static hashrate allocation")

            if self.options.initial_v27_hashrate is not None:
                self.current_v27_hashrate = self.options.initial_v27_hashrate
                self.current_v26_hashrate = 100.0 - self.options.initial_v27_hashrate
            else:
                self.current_v27_hashrate = 50.0
                self.current_v26_hashrate = 50.0

            self.pool_strategy = None

        self.log.info(f"\nInitial hashrate: v27={self.current_v27_hashrate:.1f}%, v26={self.current_v26_hashrate:.1f}%\n")

        # Load node metadata from network.yaml
        self.load_network_metadata()

        # Initialize economic node strategy (dynamic economic weight)
        try:
            import pkgutil
            econ_config_data = pkgutil.get_data('config', 'economic_nodes_config.yaml')
            econ_config = yaml.safe_load(econ_config_data.decode('utf-8'))

            economic_profiles = load_economic_nodes_from_network(
                self.node_metadata,
                econ_config,
                self.options.economic_scenario
            )

            if economic_profiles:
                self.economic_strategy = EconomicNodeStrategy(economic_profiles)

                # Calculate initial economic allocation (nodes start on their partition's fork)
                self.current_v27_economic, self.current_v26_economic = \
                    self.economic_strategy.calculate_economic_allocation(
                        time(), self.price_oracle
                    )

                # Calculate transactional weights (fee-generating activity)
                self.current_v27_transactional, self.current_v26_transactional = \
                    self.economic_strategy.get_fee_generation_weight()

                # Calculate solo miner hashrate
                self.current_v27_solo_hashrate, self.current_v26_solo_hashrate, solo_miners = \
                    self.economic_strategy.get_mining_allocation()

                econ_count = len(economic_profiles)
                econ_types = {}
                solo_miner_count = 0
                for p in economic_profiles:
                    t = p.node_type.value
                    econ_types[t] = econ_types.get(t, 0) + 1
                    if p.hashrate_pct > 0:
                        solo_miner_count += 1

                self.log.info(f"✓ Economic strategy initialized ({econ_count} nodes: {econ_types})")
                self.log.info(f"  Initial economic weight: v27={self.current_v27_economic:.1f}%, v26={self.current_v26_economic:.1f}%")
                self.log.info(f"  Initial transactional weight: v27={self.current_v27_transactional:.1f}%, v26={self.current_v26_transactional:.1f}%")
                if solo_miner_count > 0:
                    total_solo = self.current_v27_solo_hashrate + self.current_v26_solo_hashrate
                    self.log.info(f"  Solo miners: {solo_miner_count} nodes with {total_solo:.2f}% hashrate")
                    self.log.info(f"    v27: {self.current_v27_solo_hashrate:.2f}%, v26: {self.current_v26_solo_hashrate:.2f}%")
                self.log.info(f"  Update interval: {self.options.economic_update_interval}s")
            else:
                self.log.info("No economic/user nodes found in metadata, using static economic weight")
                self.current_v27_economic = self.options.v27_economic
                self.current_v26_economic = self.options.v26_economic
                # Without economic strategy, assume 50/50 transactional split
                self.current_v27_transactional = self.options.v27_economic
                self.current_v26_transactional = self.options.v26_economic
                self.economic_strategy = None

        except Exception as e:
            self.log.warning(f"Could not load economic config: {e}")
            self.log.info("Using static economic allocation from --v27-economic")
            self.current_v27_economic = self.options.v27_economic
            self.current_v26_economic = self.options.v26_economic
            self.current_v27_transactional = self.options.v27_economic
            self.current_v26_transactional = self.options.v26_economic
            self.economic_strategy = None

        # Initialize difficulty oracle (if enabled)
        if self.options.enable_difficulty:
            self.tick_interval = self.options.tick_interval
            self.difficulty_oracle = DifficultyOracle(
                target_block_interval=float(self.options.interval),
                retarget_interval=self.options.retarget_interval,
                pre_fork_difficulty=1.0,
                max_adjustment_factor=4.0,
                min_difficulty=self.options.min_difficulty,
                enable_eda=self.options.enable_eda,
            )
            self.difficulty_oracle.initialize_fork('v27', initial_height=self.options.start_height)
            self.difficulty_oracle.initialize_fork('v26', initial_height=self.options.start_height)
            self.log.info(f"Difficulty oracle initialized:")
            self.log.info(f"  Target interval: {self.options.interval}s, Retarget every {self.options.retarget_interval} blocks")
            self.log.info(f"  Tick interval: {self.tick_interval}s, Min difficulty: {self.options.min_difficulty}")
            self.log.info(f"  EDA: {'enabled' if self.options.enable_eda else 'disabled'}")

        # Partition nodes
        self.partition_nodes_by_version()

        # Identify foreign-accepting nodes for asymmetric fork propagation
        self.build_foreign_accepting_nodes()

        # Build partition peer lists for dynamic switching
        self.build_partition_peer_lists()

        # Build pool-to-node mapping
        if self.pool_strategy:
            self.build_pool_node_mapping()

        # Auto-detect starting height BEFORE reorg oracle initialization
        # This ensures lca_height matches the actual fork point
        if self.options.start_height == 101:  # Default value, auto-detect instead
            detected_heights = []
            for node in (self.v27_nodes + self.v26_nodes)[:3]:  # Sample first few nodes
                try:
                    h = node.getblockcount()
                    detected_heights.append(h)
                except:
                    pass
            if detected_heights:
                # Use minimum as common ancestor (conservative)
                self.options.start_height = min(detected_heights)
                self.log.info(f"Auto-detected start_height={self.options.start_height} from nodes")

        # Initialize reorg oracle (if enabled) - AFTER auto-detection so lca_height is correct
        if self.options.enable_reorg_metrics:
            # Get initial fork height (LCA = starting height before fork diverges)
            lca_height = self.options.start_height

            # Determine total nodes from pool strategy if available
            total_pool_nodes = len(self.pool_strategy.pools) if self.pool_strategy else 8

            self.reorg_oracle = ReorgOracle(
                lca_height=lca_height,
                lca_hash="fork-point",
                propagation_window=30.0,
                total_nodes=total_pool_nodes
            )
            # Initialize both forks from the LCA
            self.reorg_oracle.initialize_fork('v27', lca_height)
            self.reorg_oracle.initialize_fork('v26', lca_height)

            self.log.info(f"✓ Reorg oracle initialized (LCA height={lca_height})")

            # Register pool nodes with reorg oracle
            if self.pool_strategy:
                for pool_id, pool in self.pool_strategy.pools.items():
                    initial_fork = self.pool_strategy.current_allocation.get(pool_id, 'v27')
                    self.reorg_oracle.register_node(pool_id, initial_fork)
                    # Also set current_fork on pool profile for tracking
                    pool.current_fork = initial_fork
                self.log.info(f"  Registered {len(self.pool_strategy.pools)} pools with reorg oracle")

        # Main mining loop
        start_time = time()
        last_hashrate_update = start_time
        last_price_update = start_time
        last_economic_update = start_time
        last_snapshot = start_time

        # Capture initial snapshot
        self.capture_time_series_snapshot(0)

        self.log.info(f"\n{'='*70}")
        if self.difficulty_oracle:
            self.log.info(f"Starting partition mining (DIFFICULTY MODE)...")
        else:
            self.log.info(f"Starting partition mining (LEGACY MODE)...")
        self.log.info(f"{'='*70}\n")

        while time() - start_time < self.options.duration:
            current_time = time()
            elapsed = int(current_time - start_time)

            # Update economic node allocation (every 5 minutes by default)
            if self.economic_strategy and (current_time - last_economic_update >= self.options.economic_update_interval):
                old_v27_econ = self.current_v27_economic
                old_v26_econ = self.current_v26_economic

                self.current_v27_economic, self.current_v26_economic = \
                    self.economic_strategy.calculate_economic_allocation(
                        current_time, self.price_oracle
                    )

                # Update transactional weights (for fee calculations)
                self.current_v27_transactional, self.current_v26_transactional = \
                    self.economic_strategy.get_fee_generation_weight()

                # Update solo miner hashrate allocation
                old_v27_solo = self.current_v27_solo_hashrate
                old_v26_solo = self.current_v26_solo_hashrate
                self.current_v27_solo_hashrate, self.current_v26_solo_hashrate, _ = \
                    self.economic_strategy.get_mining_allocation()

                econ_change = abs(self.current_v27_economic - old_v27_econ)
                solo_change = abs(self.current_v27_solo_hashrate - old_v27_solo)

                if econ_change > 0.5:  # More than 0.5% change
                    self.log.info(f"\n ECONOMIC REALLOCATION at {elapsed}s:")
                    self.log.info(f"   v27: {old_v27_econ:.1f}% -> {self.current_v27_economic:.1f}%")
                    self.log.info(f"   v26: {old_v26_econ:.1f}% -> {self.current_v26_economic:.1f}%")
                    self.log.info(f"   Transactional: v27={self.current_v27_transactional:.1f}%, v26={self.current_v26_transactional:.1f}%")

                    # Log solo miner changes if any
                    if solo_change > 0.01:
                        self.log.info(f"   Solo miners: v27={old_v27_solo:.2f}%->{self.current_v27_solo_hashrate:.2f}%, "
                                      f"v26={old_v26_solo:.2f}%->{self.current_v26_solo_hashrate:.2f}%")

                    # Log notable node decisions
                    recent = [d for d in self.economic_strategy.decision_history[-30:]
                              if d.timestamp >= last_economic_update]
                    for decision in recent:
                        if decision.ideology_override:
                            self.log.info(f"   {decision.node_id}: staying on {decision.chosen_fork} (ideology)")
                        elif decision.inertia_held:
                            self.log.info(f"   {decision.node_id}: staying on {decision.chosen_fork} (inertia)")

                last_economic_update = current_time

            # Capture time series snapshot at regular intervals
            if current_time - last_snapshot >= self.options.snapshot_interval:
                self.capture_time_series_snapshot(elapsed)
                last_snapshot = current_time

            # Update prices (every minute by default)
            if current_time - last_price_update >= self.options.price_update_interval:
                # Use blocks_mined counters for reliable fork depth calculation
                # (node getblockcount() can be unreliable in partitioned networks)
                fork_depth = self.blocks_mined['v27'] + self.blocks_mined['v26']

                # Still get heights for price oracle (but fork_depth is what matters for sustained check)
                v27_height = self.options.start_height + self.blocks_mined['v27']
                v26_height = self.options.start_height + self.blocks_mined['v26']

                # Chainwork-based chain weights if difficulty oracle available
                v27_cw_override = None
                v26_cw_override = None
                if self.difficulty_oracle:
                    v27_cw_override = self.difficulty_oracle.get_chain_weight('v27')
                    v26_cw_override = self.difficulty_oracle.get_chain_weight('v26')

                # Debug: log price update inputs
                if self.options.debug_prices:
                    self.log.info(f"  [PRICE UPDATE] fork_depth={fork_depth}, sustained={self.price_oracle.fork_sustained}")
                    self.log.info(f"  [PRICE UPDATE] chain_weights: v27={v27_cw_override}, v26={v26_cw_override}")
                    self.log.info(f"  [PRICE UPDATE] econ: v27={self.current_v27_economic}%, v26={self.current_v26_economic}%")
                    self.log.info(f"  [PRICE UPDATE] hash: v27={self.current_v27_hashrate}%, v26={self.current_v26_hashrate}%")

                old_v27_price = self.price_oracle.get_price('v27')
                old_v26_price = self.price_oracle.get_price('v26')

                self.price_oracle.update_prices_from_state(
                    v27_height=v27_height,
                    v26_height=v26_height,
                    v27_economic_pct=self.current_v27_economic,
                    v26_economic_pct=self.current_v26_economic,
                    v27_hashrate_pct=self.current_v27_hashrate,
                    v26_hashrate_pct=self.current_v26_hashrate,
                    common_ancestor_height=self.options.start_height,
                    v27_chain_weight_override=v27_cw_override,
                    v26_chain_weight_override=v26_cw_override,
                )

                new_v27_price = self.price_oracle.get_price('v27')
                new_v26_price = self.price_oracle.get_price('v26')

                if self.options.debug_prices:
                    self.log.info(f"  [PRICE UPDATE] prices: v27=${old_v27_price:,.0f}->${new_v27_price:,.0f}, v26=${old_v26_price:,.0f}->${new_v26_price:,.0f}")

                # Update fees based on network state
                if self.difficulty_oracle:
                    v27_blocks_per_hour = self.difficulty_oracle.get_blocks_per_hour('v27', self.current_v27_hashrate)
                    v26_blocks_per_hour = self.difficulty_oracle.get_blocks_per_hour('v26', self.current_v26_hashrate)
                else:
                    v27_blocks_per_hour = (self.blocks_mined['v27'] / max(1, elapsed/3600))
                    v26_blocks_per_hour = (self.blocks_mined['v26'] / max(1, elapsed/3600))

                self.fee_oracle.update_fees_from_state(
                    v27_blocks_per_hour=v27_blocks_per_hour,
                    v26_blocks_per_hour=v26_blocks_per_hour,
                    v27_economic_pct=self.current_v27_economic,
                    v26_economic_pct=self.current_v26_economic,
                    price_oracle=self.price_oracle,
                    difficulty_oracle=self.difficulty_oracle,
                    v27_hashrate_pct=self.current_v27_hashrate,
                    v26_hashrate_pct=self.current_v26_hashrate,
                    # Pass transactional weights for fee calculation
                    # Fees are driven by transaction activity, not custody holdings
                    v27_transactional_pct=self.current_v27_transactional,
                    v26_transactional_pct=self.current_v26_transactional,
                )

                last_price_update = current_time

            # Update hashrate allocation (every 10 minutes by default)
            if self.pool_strategy and (current_time - last_hashrate_update >= self.options.hashrate_update_interval):

                # Pools make decisions
                old_v27_hash = self.current_v27_hashrate
                old_v26_hash = self.current_v26_hashrate

                self.current_v27_hashrate, self.current_v26_hashrate = \
                    self.pool_strategy.calculate_hashrate_allocation(
                        current_time, self.price_oracle, self.fee_oracle,
                        difficulty_oracle=self.difficulty_oracle,
                    )

                # Ensure fork heights are current before detecting reorgs
                if self.reorg_oracle:
                    self.reorg_oracle.update_fork_heights(
                        v27_height=self.options.start_height + self.blocks_mined['v27'],
                        v26_height=self.options.start_height + self.blocks_mined['v26']
                    )

                # Detect fork switches and record reorgs
                if self.reorg_oracle:
                    for pool_id, pool in self.pool_strategy.pools.items():
                        old_fork = pool.current_fork
                        new_fork = self.pool_strategy.current_allocation.get(pool_id, old_fork)

                        if old_fork != new_fork:
                            # Pool is switching forks - this causes a reorg!
                            reorg_event = self.reorg_oracle.record_fork_switch(
                                node_id=pool_id,
                                old_fork=old_fork,
                                new_fork=new_fork,
                                sim_time=elapsed
                            )
                            self.log.info(f"  REORG: {pool_id} switched {old_fork}->{new_fork}, "
                                          f"depth={reorg_event.depth}, orphaned={len(reorg_event.blocks_invalidated)} blocks")

                        # Update pool's current_fork tracking
                        pool.current_fork = new_fork

                # Log significant changes
                hash_change = abs(self.current_v27_hashrate - old_v27_hash)
                if hash_change > 1.0:  # More than 1% change
                    self.log.info(f"\n HASHRATE REALLOCATION at {elapsed}s:")
                    self.log.info(f"   v27: {old_v27_hash:.1f}% -> {self.current_v27_hashrate:.1f}%")
                    self.log.info(f"   v26: {old_v26_hash:.1f}% -> {self.current_v26_hashrate:.1f}%")

                    # Log pool switches
                    recent_decisions = [d for d in self.pool_strategy.decision_history[-20:]
                                      if d.timestamp >= last_hashrate_update]

                    for decision in recent_decisions:
                        if decision.ideology_override:
                            self.log.info(f"   {decision.pool_id}: mining {decision.chosen_fork} "
                                        f"despite ${decision.opportunity_cost_usd:,.0f} loss (ideology)")
                        elif decision.chosen_fork != decision.rational_choice:
                            self.log.info(f"   {decision.pool_id}: forced to switch to {decision.chosen_fork}")

                last_hashrate_update = current_time

                # Evaluate economic/user node partition switches
                # These nodes may switch partitions based on price, ideology, etc.
                if self.options.enable_dynamic_switching:
                    self.evaluate_economic_node_switches(elapsed)

            # === BLOCK PRODUCTION ===
            if self.difficulty_oracle:
                # DIFFICULTY MODE: per-tick probability for each fork independently
                sim_elapsed = elapsed  # Use wall-clock elapsed as sim_time

                for fork_id in ['v27', 'v26']:
                    hashrate_pct = self.current_v27_hashrate if fork_id == 'v27' else self.current_v26_hashrate

                    if self.difficulty_oracle.should_mine_block(fork_id, hashrate_pct, self.tick_interval):
                        miner = self._select_miner_for_fork(fork_id)
                        if not miner:
                            continue

                        try:
                            miner_wallet = Commander.ensure_miner(miner)
                            address = miner_wallet.getnewaddress()
                            self.generatetoaddress(miner, 1, address, sync_fun=self.no_op)

                            # Asymmetric fork: push v27 blocks into the v26 island
                            self.propagate_to_foreign_accepting(miner, fork_id)

                            self.blocks_mined[fork_id] += 1
                            new_height = (self.v27_nodes[0].getblockcount() if fork_id == 'v27' and self.v27_nodes
                                          else self.v26_nodes[0].getblockcount() if fork_id == 'v26' and self.v26_nodes
                                          else self.options.start_height + self.blocks_mined[fork_id])

                            # Record block with reorg oracle
                            if self.reorg_oracle:
                                pool_id = self.get_node_pool_id(miner)
                                if pool_id:
                                    self.reorg_oracle.record_block_mined(pool_id, fork_id, new_height)
                                # Update fork heights
                                self.reorg_oracle.update_fork_heights(
                                    v27_height=self.options.start_height + self.blocks_mined['v27'],
                                    v26_height=self.options.start_height + self.blocks_mined['v26']
                                )

                            retarget_event = self.difficulty_oracle.record_block(fork_id, sim_elapsed, new_height)

                            if retarget_event:
                                eda_str = " (EDA)" if retarget_event.is_eda else ""
                                self.log.info(
                                    f"  >> {fork_id} RETARGET{eda_str} at {elapsed}s: "
                                    f"difficulty {retarget_event.old_difficulty:.6f} -> {retarget_event.new_difficulty:.6f} "
                                    f"(factor={retarget_event.adjustment_factor:.3f})"
                                )

                            # Log the block
                            v27_price = self.price_oracle.get_price('v27')
                            v26_price = self.price_oracle.get_price('v26')
                            fork_status = "SUSTAINED" if self.price_oracle.fork_sustained else "natural split"
                            diff_state = self.difficulty_oracle.forks[fork_id]
                            pool_id = self.get_node_pool_id(miner)
                            pool_str = f"({pool_id})" if pool_id else ""

                            self.log.info(
                                f"[{elapsed:4d}s] {fork_id} block {pool_str:15s} | "
                                f"Blks: v27={self.blocks_mined['v27']:3d} v26={self.blocks_mined['v26']:3d} | "
                                f"Diff: {diff_state.current_difficulty:.4f} | "
                                f"Hash: {self.current_v27_hashrate:4.1f}%/{self.current_v26_hashrate:4.1f}% | "
                                f"Price: ${v27_price:,.0f}/${v26_price:,.0f} [{fork_status}]"
                            )

                        except Exception as e:
                            self.log.error(f"Error mining {fork_id} block: {e}")

                sleep(self.tick_interval)

            else:
                # LEGACY MODE: one block per interval, probabilistic fork selection
                miner, partition = self.select_mining_node()

                if not miner:
                    self.log.warning("No miner available, skipping block")
                    sleep(self.options.interval)
                    continue

                try:
                    miner_wallet = Commander.ensure_miner(miner)
                    address = miner_wallet.getnewaddress()
                    self.generatetoaddress(miner, 1, address, sync_fun=self.no_op)

                    # Asymmetric fork: push v27 blocks into the v26 island
                    self.propagate_to_foreign_accepting(miner, partition)

                    self.blocks_mined[partition] += 1

                    v27_height = self.v27_nodes[0].getblockcount() if self.v27_nodes else 0
                    v26_height = self.v26_nodes[0].getblockcount() if self.v26_nodes else 0
                    fork_depth = v27_height + v26_height - (2 * self.options.start_height)

                    # Record block with reorg oracle
                    if self.reorg_oracle:
                        pool_id = self.get_node_pool_id(miner)
                        if pool_id:
                            block_height = v27_height if partition == 'v27' else v26_height
                            self.reorg_oracle.record_block_mined(pool_id, partition, block_height)
                        # Update fork heights
                        self.reorg_oracle.update_fork_heights(v27_height, v26_height)

                    # Get current state
                    v27_price = self.price_oracle.get_price('v27')
                    v26_price = self.price_oracle.get_price('v26')
                    price_ratio = v27_price / v26_price if v26_price > 0 else 1.0

                    fork_status = "SUSTAINED" if self.price_oracle.fork_sustained else "natural split"

                    # Get pool name for logging
                    pool_id = self.get_node_pool_id(miner)
                    pool_str = f"({pool_id})" if pool_id else ""

                    self.log.info(
                        f"[{elapsed:4d}s] {partition} block {pool_str:15s} | "
                        f"Heights: v27={v27_height:3d} v26={v26_height:3d} | "
                        f"Hash: {self.current_v27_hashrate:4.1f}%/{self.current_v26_hashrate:4.1f}% | "
                        f"Econ: {self.current_v27_economic:4.1f}%/{self.current_v26_economic:4.1f}% | "
                        f"Price: ${v27_price:,.0f}/${v26_price:,.0f} [{fork_status}]"
                    )

                except Exception as e:
                    self.log.error(f"Error mining block: {e}")

                sleep(self.options.interval)

        # Fork reunion (before final summary so we can log it inline)
        reunion_results = self.reunite_forks()

        # Final summary
        elapsed_min = (time() - start_time) / 60.0
        total_blocks = self.blocks_mined['v27'] + self.blocks_mined['v26']

        self.log.info(f"\n{'='*70}")
        self.log.info(f"Partition Mining Complete")
        self.log.info(f"{'='*70}")
        self.log.info(f"Duration: {elapsed_min:.2f} minutes")
        self.log.info(f"Blocks mined: v27={self.blocks_mined['v27']}, v26={self.blocks_mined['v26']}, total={total_blocks}")

        # Final prices
        v27_price = self.price_oracle.get_price('v27')
        v26_price = self.price_oracle.get_price('v26')

        self.log.info(f"\nFinal State:")
        self.log.info(f"  Prices: v27=${v27_price:,.0f}, v26=${v26_price:,.0f}")
        self.log.info(f"  Hashrate: v27={self.current_v27_hashrate:.1f}%, v26={self.current_v26_hashrate:.1f}%")
        self.log.info(f"  Economic: v27={self.current_v27_economic:.1f}%, v26={self.current_v26_economic:.1f}%")

        # Difficulty oracle summary
        if self.difficulty_oracle:
            self.log.info("")
            winner_id, winner_cw, loser_cw = self.difficulty_oracle.get_winning_fork()
            self.log.info(f"  Difficulty Model:")
            self.log.info(f"    Winning fork: {winner_id} (chainwork: {winner_cw:.4f} vs {loser_cw:.4f})")
            for fid, state in self.difficulty_oracle.forks.items():
                self.log.info(f"    {fid}: difficulty={state.current_difficulty:.6f}, "
                            f"chainwork={state.cumulative_chainwork:.4f}, "
                            f"chain_weight={self.difficulty_oracle.get_chain_weight(fid):.4f}")
            adj_count = len(self.difficulty_oracle.adjustment_history)
            eda_count = sum(1 for e in self.difficulty_oracle.adjustment_history if e.is_eda)
            self.log.info(f"    Retargets: {adj_count} (EDA: {eda_count})")

        # Pool strategy summary
        if self.pool_strategy:
            self.log.info("")
            self.pool_strategy.print_allocation_summary()

        # Economic strategy summary
        if self.economic_strategy:
            self.log.info("")
            self.economic_strategy.print_allocation_summary()

        # Dynamic partition switching summary
        if self.partition_switch_history:
            self.log.info("")
            self.log.info("=" * 50)
            self.log.info("DYNAMIC PARTITION SWITCHES")
            self.log.info("=" * 50)
            self.log.info(f"  Total switches: {len(self.partition_switch_history)}")

            # Count by direction
            v27_to_v26 = sum(1 for s in self.partition_switch_history if s['from'] == 'v27')
            v26_to_v27 = sum(1 for s in self.partition_switch_history if s['from'] == 'v26')
            self.log.info(f"  v27 -> v26: {v27_to_v26}")
            self.log.info(f"  v26 -> v27: {v26_to_v27}")

            # Final partition distribution
            v27_count = sum(1 for p in self.node_current_partition.values() if p == 'v27')
            v26_count = sum(1 for p in self.node_current_partition.values() if p == 'v26')
            self.log.info(f"  Final distribution: v27={v27_count} nodes, v26={v26_count} nodes")

            # List recent switches
            self.log.info("\n  Recent switches:")
            for switch in self.partition_switch_history[-10:]:
                self.log.info(f"    {switch['node']}: {switch['from']} -> {switch['to']} ({switch['reason']})")

        # Reorg oracle summary
        if self.reorg_oracle:
            # Calculate reunion analysis
            if self.difficulty_oracle:
                v27_chainwork = self.difficulty_oracle.get_cumulative_chainwork('v27')
                v26_chainwork = self.difficulty_oracle.get_cumulative_chainwork('v26')
            else:
                v27_chainwork = float(self.blocks_mined['v27'])
                v26_chainwork = float(self.blocks_mined['v26'])

            reunion = self.reorg_oracle.calculate_reunion_reorg(v27_chainwork, v26_chainwork)

            self.log.info("")
            self.reorg_oracle.print_summary()

            self.log.info(f"\nReunion Analysis (hypothetical partition merge):")
            self.log.info(f"  Winning fork: {reunion['winning_fork']}")
            self.log.info(f"  Losing fork depth: {reunion['losing_fork_depth']} blocks")
            self.log.info(f"  Nodes on losing fork: {reunion['num_nodes_on_losing_fork']}")
            self.log.info(f"  Reunion reorg mass: {reunion['reunion_reorg_mass']}")
            self.log.info(f"  Additional orphans on reunion: {reunion['additional_orphans']}")

        # Export results
        try:
            import json
            import base64
            from datetime import datetime

            # Generate results ID if not provided
            results_id = self.options.results_id
            if not results_id:
                results_id = datetime.now().strftime("%Y%m%d_%H%M%S")

            # Build consolidated results object
            consolidated_results = {
                'metadata': {
                    'results_id': results_id,
                    'timestamp': datetime.now().isoformat(),
                    'duration_seconds': self.options.duration,
                    'pool_scenario': self.options.pool_scenario,
                    'economic_scenario': self.options.economic_scenario,
                    'difficulty_enabled': self.options.enable_difficulty,
                    'reorg_metrics_enabled': self.options.enable_reorg_metrics,
                    'interval': self.options.interval,
                    'start_height': self.options.start_height,
                },
                'summary': {
                    'blocks_mined': dict(self.blocks_mined),
                    'total_blocks': self.blocks_mined['v27'] + self.blocks_mined['v26'],
                    'final_hashrate': {
                        'v27': self.current_v27_hashrate,
                        'v26': self.current_v26_hashrate,
                    },
                    'final_economic': {
                        'v27': self.current_v27_economic,
                        'v26': self.current_v26_economic,
                    },
                    'final_prices': {
                        'v27': self.price_oracle.get_price('v27'),
                        'v26': self.price_oracle.get_price('v26'),
                    },
                },
                'time_series': self.time_series,  # For charting
                'partition_switches': {
                    'total_switches': len(self.partition_switch_history),
                    'switches': self.partition_switch_history,
                    'final_partition_state': dict(self.node_current_partition),
                },
            }

            # Capture final snapshot
            self.capture_time_series_snapshot(int(time() - start_time))

            # Add oracle exports
            if self.pool_strategy:
                self.pool_strategy.export_to_json('/tmp/partition_pools.json')
                # Read back for consolidated export
                with open('/tmp/partition_pools.json', 'r') as f:
                    consolidated_results['pools'] = json.load(f)

            if self.economic_strategy:
                self.economic_strategy.export_to_json('/tmp/partition_economic.json')
                with open('/tmp/partition_economic.json', 'r') as f:
                    consolidated_results['economic'] = json.load(f)

            self.price_oracle.export_to_json('/tmp/partition_prices.json')
            with open('/tmp/partition_prices.json', 'r') as f:
                consolidated_results['prices'] = json.load(f)

            self.fee_oracle.export_to_json('/tmp/partition_fees.json')
            with open('/tmp/partition_fees.json', 'r') as f:
                consolidated_results['fees'] = json.load(f)

            if self.difficulty_oracle:
                self.difficulty_oracle.export_to_json('/tmp/partition_difficulty.json')
                with open('/tmp/partition_difficulty.json', 'r') as f:
                    consolidated_results['difficulty'] = json.load(f)

            if self.reorg_oracle:
                reorg_export = self.reorg_oracle.export_to_json()
                if self.difficulty_oracle:
                    v27_cw = self.difficulty_oracle.get_cumulative_chainwork('v27')
                    v26_cw = self.difficulty_oracle.get_cumulative_chainwork('v26')
                else:
                    v27_cw = float(self.blocks_mined['v27'])
                    v26_cw = float(self.blocks_mined['v26'])
                reorg_export['reunion_analysis'] = self.reorg_oracle.calculate_reunion_reorg(v27_cw, v26_cw)
                consolidated_results['reorg'] = reorg_export
                with open('/tmp/partition_reorg.json', 'w') as f:
                    json.dump(reorg_export, f, indent=2)

            # Fork reunion results
            consolidated_results['reunion'] = reunion_results

            # Save consolidated results to file
            with open('/tmp/partition_results.json', 'w') as f:
                json.dump(consolidated_results, f, indent=2)

            self.log.info(f"\n Results exported to /tmp/partition_*.json")
            self.log.info(f" Results ID: {results_id}")

            # Output base64-encoded results to logs for extraction
            # This survives pod termination since it's in the logs
            results_json = json.dumps(consolidated_results)
            results_b64 = base64.b64encode(results_json.encode()).decode()

            self.log.info(f"\n{'='*70}")
            self.log.info("RESULTS_EXPORT_START")
            self.log.info(f"RESULTS_ID:{results_id}")
            # Split into chunks for readability (some log systems have line limits)
            chunk_size = 1000
            for i in range(0, len(results_b64), chunk_size):
                self.log.info(f"RESULTS_DATA:{results_b64[i:i+chunk_size]}")
            self.log.info("RESULTS_EXPORT_END")
            self.log.info(f"{'='*70}")

        except Exception as e:
            self.log.error(f"Error exporting results: {e}")
            import traceback
            self.log.error(traceback.format_exc())

        self.log.info(f"{'='*70}\n")


def main():
    """Entry point for partition mining with pools"""
    PartitionMinerWithPools("").main()


if __name__ == "__main__":
    main()
